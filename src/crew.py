from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool, ScrapeWebsiteTool
from crewai.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field
from typing import Type



#This script is a research agent that uses multiple agents to perform web research.
#It uses the CrewAI library to create a crew of agents that work together to analyze a user's query, 
#search the web, scrape websites, evaluate content, and generate a final report.


#Initialize the LLM with your OpenAI API key
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
    #openai_api_key=os.environ["OPENAI_API_KEY"],
    # other params...
)

#This class is used to define the input schema for the GPTContentAnalyzer tool.
#It uses Pydantic to define the input fields and their descriptions to prevent errors.
class GPTContentAnalyzerInput(BaseModel):
    """GPTContentAnalyzerInput"""
    query: str = Field(description="The user's research query.")
    content: str = Field(description="The content generated by the agent.")


#Content Analyzer Tool
# This tool uses the LLM to analyze content for relevance, credibility, and accuracy against a research query.
class GPTContentAnalyzer(BaseTool):
    """GPTContentAnalyzer"""
    name: str = "GPTContentAnalyzer"
    description: str =  "Analyzes content for relevance, credibility, and accuracy against a research query."
    args_schema: Type[BaseModel] = GPTContentAnalyzerInput

    def _run(self, content: str, query: str):
        #The LLM uses the user's query and the content generated by the agent to evaluate the relevance, credibility, and accuracy of the content.
        prompt = PromptTemplate.from_template("""
        You are an expert research assistant. A user has asked a question: "{query}".

        Here is a piece of content:

        {content}

        Evaluate the content and return:
        1. Relevance (High, Medium, Low)
        2. Credibility (High, Medium, Low)
        3. Key insights (bullet points)
        4. Any potential bias or reliability issues
        """)

        chain = prompt | llm

        return chain.invoke({"content": content[:3000], "query": query})

#Initialize the tools
analyzer_tool = GPTContentAnalyzer()
#Search Tool for retrieving relevant information from the web
search_tool = SerperDevTool()
#Scrape Tool for scraping websites
scrape_tool = ScrapeWebsiteTool()

# This is the main class for the WebResearchAgent crew.
@CrewBase
class WebResearchAgent():
    """WebResearchAgent crew"""

    #load the agent and task configurations from YAML files
    agents_config = 'config/agents.yaml'
    tasks_config = 'config/tasks.yaml'

    # Initialize the agents
    @agent
    def query_analyst(self) -> Agent:
        """Query Analyst Agent
        This agent is responsible for analyzing the user's query and breaking it down into smaller tasks.
        It uses the LLM to understand the query and generate a structured plan for the research process.
        """
        return Agent(
            config=self.agents_config['query_analyst'],
            #llm=llm,
            verbose=True
        )
    
    @agent
    def search_expert(self) -> Agent:
        """Search Expert Agent
        This agent is responsible for searching the web for relevant information based on the user's query.
        It uses the Serper API to retrieve search results and relevant links.
        Tool Access: Serper Web Search Tool
        """
        return Agent(
            config=self.agents_config['search_expert'],
            #llm=llm,
            tools=[search_tool],
            verbose=True
        )
    
    @agent
    def web_scraper(self) -> Agent:
        """Web Scraper Agent
        This agent is responsible for scraping the content from the web pages retrieved by the Search Expert Agent.
        It uses the ScrapeWebsiteTool to extract relevant information from the web pages.
        Tool Access: ScrapeWebsiteTool
        """
        return Agent(
            config=self.agents_config['web_scraper'],
            #llm=llm,
            tools=[scrape_tool],
            verbose=True
        )
    
    @agent
    def content_evaluator(self) -> Agent:
        """Content Evaluator Agent
        This agent is responsible for evaluating the content scraped by the Web Scraper Agent.
        It uses the GPTContentAnalyzer tool to analyze the content for relevance, credibility, and accuracy.
        Tool Access: GPTContentAnalyzer
        """
        return Agent(
            config=self.agents_config['content_evaluator'],
            #llm=llm,
            tools=[analyzer_tool],
            verbose=True
        )
    
    @agent
    def synthesizer(self) -> Agent:
        """Synthesizer Agent
        This agent is responsible for synthesizing the information gathered by the other agents.
        It uses the LLM to generate a coherent and comprehensive report based on the analyzed content.
        The report is structured and includes key insights, relevance, and credibility ratings.
        """
        return Agent(
            config=self.agents_config['synthesizer'],
            #llm=llm,
            verbose=True
        )

    @agent
    def reporting_analyst(self) -> Agent:
        """Reporting Analyst Agent
        This agent is responsible for generating the final report based on the synthesized information.
        It uses the LLM to create a well-structured report that includes all the key insights and findings from the research process.
        """
        return Agent(
            config=self.agents_config['reporting_analyst'],
            #llm=llm,
            verbose=True
        )

    # Initialize the tasks
    @task
    def query_task(self) -> Task:
        """Query Task
        This task is responsible for analyzing the user's query and breaking it down into smaller tasks.
        """
        return Task(
            config=self.tasks_config['query_task'],
        )
    @task
    def search_task(self) -> Task:
        """Search Task
        This task is responsible for searching the web for relevant information based on the user's query.
        """
        return Task(
            config=self.tasks_config['search_task'],
        )
    @task
    def scrape_task(self) -> Task:
        """Scrape Task
        This task is responsible for scraping the content from the web pages retrieved by the Search Expert Agent.
        """
        return Task(
            config=self.tasks_config['scrape_task'],
        )
    @task
    def evaluate_task(self) -> Task:
        """Evaluate Task
        This task is responsible for evaluating the content scraped by the Web Scraper Agent.
        """
        return Task(
            config=self.tasks_config['evaluate_task'],
        )
    @task
    def synthesize_task(self) -> Task:
        """Synthesize Task
        This task is responsible for synthesizing the information gathered by the other agents.
        """
        return Task(
            config=self.tasks_config['synthesize_task'],
        )

    @task
    def reporting_task(self) -> Task:
        """Reporting Task
        This task is responsible for generating the final report based on the synthesized information.
        """
        return Task(
            config=self.tasks_config['reporting_task'],
            #output_file='report.md'
        )

    @crew
    def crew(self) -> Crew:
        """Creates the WebResearchAgent crew"""


        return Crew(
            agents=self.agents, # Automatically created by the @agent decorator
            tasks=self.tasks, # Automatically created by the @task decorator
            process=Process.sequential, # Sequential process indicates that tasks are executed in order
            verbose=True,
        )

